{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import neuralNet\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import random\n",
    "import homology as hm\n",
    "import util\n",
    "import datetime\n",
    "\n",
    "import importlib"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Network\n",
    "\n",
    "This notebook will contain an example use of the PersLay implementation. The task here is to distinguish whether a dataset is generated by ```make_blobs()``` or ```make_circles()```.\n",
    "\n",
    "As a first step we create our labeled data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 out of 200 datasets created.\n",
      "11 out of 200 datasets created.\n",
      "21 out of 200 datasets created.\n",
      "31 out of 200 datasets created.\n",
      "41 out of 200 datasets created.\n",
      "51 out of 200 datasets created.\n",
      "61 out of 200 datasets created.\n",
      "71 out of 200 datasets created.\n",
      "81 out of 200 datasets created.\n",
      "91 out of 200 datasets created.\n",
      "101 out of 200 datasets created.\n",
      "111 out of 200 datasets created.\n",
      "121 out of 200 datasets created.\n",
      "131 out of 200 datasets created.\n",
      "141 out of 200 datasets created.\n",
      "151 out of 200 datasets created.\n",
      "161 out of 200 datasets created.\n",
      "171 out of 200 datasets created.\n",
      "181 out of 200 datasets created.\n",
      "191 out of 200 datasets created.\n",
      "1 out of 25 datasets created.\n",
      "11 out of 25 datasets created.\n",
      "21 out of 25 datasets created.\n"
     ]
    }
   ],
   "source": [
    "# Create datasets labeled\n",
    "def create_datasets(train_entries, test_entries):\n",
    "    train_labels = []\n",
    "    train_dataset = []\n",
    "    test_labels = []\n",
    "    test_dataset = []\n",
    "\n",
    "    for i in range(train_entries):\n",
    "        if random.random() >= .5:\n",
    "            circles, _ = datasets.make_circles(n_samples=20, random_state=42, factor=.5)\n",
    "            train_labels.append(0)\n",
    "\n",
    "            distances = util.create_distance_matrix(circles)\n",
    "            diagram, l = hm.rips(distances, dimensions=1)\n",
    "            diagram = hm.filtered_complexes_to_tuples(diagram, l)\n",
    "            diagram[diagram == np.inf] = 10\n",
    "\n",
    "            train_dataset.append(diagram)\n",
    "\n",
    "        else:\n",
    "            blobs, _ = datasets.make_blobs(n_samples=20, random_state=42, center_box=(1, 1), centers=3)\n",
    "            train_labels.append(1)\n",
    "\n",
    "            distances = util.create_distance_matrix(blobs)\n",
    "            diagram, l = hm.rips(distances, dimensions=1)\n",
    "            diagram = hm.filtered_complexes_to_tuples(diagram, l)\n",
    "            diagram[diagram == np.inf] = 10\n",
    "\n",
    "            train_dataset.append(diagram)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"{i + 1} out of {train_entries} datasets created.\")\n",
    "\n",
    "    for i in range(test_entries):\n",
    "        if random.random() >= .5:\n",
    "            circles, _ = datasets.make_circles(n_samples=20, random_state=42, factor=.5)\n",
    "            test_labels.append(0)\n",
    "\n",
    "            distances = util.create_distance_matrix(circles)\n",
    "            diagram, l = hm.rips(distances, dimensions=1)\n",
    "            diagram = hm.filtered_complexes_to_tuples(diagram, l)\n",
    "            diagram[diagram == np.inf] = 10\n",
    "\n",
    "            test_dataset.append(diagram)\n",
    "\n",
    "        else:\n",
    "            blobs, _ = datasets.make_blobs(n_samples=20, random_state=42, center_box=(1, 1), centers=3)\n",
    "            test_labels.append(1)\n",
    "\n",
    "            distances = util.create_distance_matrix(blobs)\n",
    "            diagram, l = hm.rips(distances, dimensions=1)\n",
    "            diagram = hm.filtered_complexes_to_tuples(diagram, l)\n",
    "            diagram[diagram == np.inf] = 10\n",
    "\n",
    "            test_dataset.append(diagram)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"{i + 1} out of {test_entries} datasets created.\")\n",
    "\n",
    "    return train_dataset, train_labels, test_dataset, test_labels\n",
    "\n",
    "\n",
    "train_dataset, train_labels, test_dataset, test_labels = create_datasets(200,25)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We create a Dataset instance to use the data with the Pytorch dataloader:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 1, 191, 2])\n",
      "torch.Size([200])\n",
      "torch.Size([25, 1, 191, 2])\n",
      "torch.Size([25])\n"
     ]
    }
   ],
   "source": [
    "class TorchDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.unsqueeze(torch.tensor(data),1)\n",
    "        print(self.data.shape)\n",
    "        self.labels = torch.tensor(labels)\n",
    "        print(self.labels.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "train_data = TorchDataset(train_dataset, train_labels)\n",
    "test_data = TorchDataset(test_dataset, test_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we need a function to train our PersLay:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "outputs": [],
   "source": [
    "# Altered this function from the deep learning course. It trains a Pytorch model using a given optimizer and data_loader\n",
    "def train(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    n_batch = len(train_loader)\n",
    "    losses_train = []\n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        loss_train = 0.0\n",
    "        for x, labels in train_loader:\n",
    "\n",
    "            x = x.to(dtype=torch.double)\n",
    "            labels = labels\n",
    "            outputs = model(x)\n",
    "\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        losses_train.append(loss_train / n_batch)\n",
    "\n",
    "        print('{}  |  Epoch {}  |  Training loss {:.5f}'.format(\n",
    "            datetime.datetime.now().time(), epoch, loss_train / n_batch))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initialize all variables:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "outputs": [],
   "source": [
    "importlib.reload(neuralNet)  # For debugging, reload the import each run\n",
    "\n",
    "model = neuralNet.PersLay(2)\n",
    "train_loader = DataLoader(train_data, batch_size=20, shuffle=True)\n",
    "optimizer = optim.SGD(model.parameters(), lr=.01)\n",
    "n_epochs = 10\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:32:39.362918  |  Epoch 1  |  Training loss 0.69183\n",
      "00:32:39.394907  |  Epoch 2  |  Training loss 0.68165\n",
      "00:32:39.425898  |  Epoch 3  |  Training loss 0.67964\n",
      "00:32:39.451888  |  Epoch 4  |  Training loss 0.67817\n",
      "00:32:39.482878  |  Epoch 5  |  Training loss 0.67620\n",
      "00:32:39.523866  |  Epoch 6  |  Training loss 0.66669\n",
      "00:32:39.561853  |  Epoch 7  |  Training loss 0.66497\n",
      "00:32:39.599840  |  Epoch 8  |  Training loss 0.65778\n",
      "00:32:39.628831  |  Epoch 9  |  Training loss 0.65446\n",
      "00:32:39.657823  |  Epoch 10  |  Training loss 0.64973\n"
     ]
    }
   ],
   "source": [
    "train(n_epochs, optimizer, model, loss_fn, train_loader)  # Train the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we see the model does learn a little bit, but not too much, maybe because the dataset isn't all that big, maybe because we only use 1 PersLay, with no additional neural network layers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "outputs": [],
   "source": [
    "def compute_accuracy(model, loader): # Again an altered function from the deep learning course.\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, labels in loader:\n",
    "            data = data.to(dtype=torch.double)\n",
    "            labels = labels\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += labels.shape[0]\n",
    "            correct += int((predicted == labels).sum())\n",
    "\n",
    "    acc =  correct / total\n",
    "    print(\"Accuracy: {:.2f}\".format(acc))\n",
    "    return acc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_data, batch_size=25, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we calculate the accuracy of the model:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.56\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.56"
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(model, test_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It's not great, but at least a bit better than flipping a coin."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}